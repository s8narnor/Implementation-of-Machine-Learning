# -*- coding: utf-8 -*-
"""ML_Assignment_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NB0JVXm-jAiAfFkGUI9_vGeRegppdZgq
"""

# Assignment 1: Implementing Linear Regression from Scratch using Gradient Descent
# Dataset: California Housing
# Name: Sarthak Narnor
# Roll No: 1220

# Resources Used:
# - scikit-learn documentation for dataset loading and preprocessing.
# - pandas documentation for DataFrame creation.
# - matplotlib documentation for ploting graphs
# - GeeksforGeeks for Gradient Descent implementation.

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# --- Implementation of the Linear Regression Model ---

class LinearRegression:
    """
    A from-scratch implementation of a Linear Regression model
    using Gradient Descent.
    """
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        """
        Initializes the model hyperparameters.
        Args:
            learning_rate (float): The step size for Gradient Descent.
            n_iterations (int): The number of passes over the training dataset.
        """
        self.lr = learning_rate
        self.n_iter = n_iterations
        self.weights = None
        self.bias = None
        self.cost_history = []

    def fit(self, X, y):
        """
        Trains the model using the Gradient Descent algorithm.
        Args:
            X (np.array): Training data features of shape (n_samples, n_features).
            y (np.array): Training data targets of shape (n_samples,).
        """
        n_samples, n_features = X.shape

        # Initialize parameters
        self.weights = np.zeros(n_features)
        self.bias = 0

        # Gradient Descent loop
        for i in range(self.n_iter):
            # Calculate predictions
            y_predicted = np.dot(X, self.weights) + self.bias

            # Calculate cost (MSE) for this iteration and store it
            cost = (1 / n_samples) * np.sum((y_predicted - y)**2)
            self.cost_history.append(cost)

            # Calculate gradients based on the formulas
            dw = (2 / n_samples) * np.dot(X.T, (y_predicted - y))
            db = (2 / n_samples) * np.sum(y_predicted - y)

            # Update weights and bias
            self.weights -= self.lr * dw
            self.bias -= self.lr * db

    def predict(self, X):
        """
        Makes predictions on new data using the learned parameters.
        Args:
            X (np.array): Data to make predictions on.
        Returns:
            np.array: The predicted values.
        """
        return np.dot(X, self.weights) + self.bias


# --- Main script execution ---

if __name__ == '__main__':
    # Dataset Preparation
    housing = fetch_california_housing()
    print(housing.DESCR)
    X, y = housing.data, housing.target

    # Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Data Preprocessing (Standardization)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test) # Use the same scaler fitted on the training data

    # Training the Model
    # Chosen hyperparameters
    learning_rate = 0.01
    iterations = 1000

    model = LinearRegression(learning_rate=learning_rate, n_iterations=iterations)
    model.fit(X_train_scaled, y_train)

    print("--- Model Training Summary ---")
    print(f"Learning Rate (alpha): {learning_rate}")
    print(f"Number of Iterations: {iterations}")
    print("Training finished.")
    print("Training completed.")
    print(f"Learned Weights: {model.weights}")
    print(f"Learned Bias: {model.bias:.4f}")

    # Evaluation and Visualization ---

    # Model Evaluation
    y_pred = model.predict(X_test_scaled)

    # Calculate Mean Squared Error (MSE)
    mse = np.mean((y_test - y_pred)**2)

    # Calculate R-squared (R2) Score
    ss_total = np.sum((y_test - np.mean(y_test))**2)
    ss_residual = np.sum((y_test - y_pred)**2)
    r2 = 1 - (ss_residual / ss_total)

    print("\n--- Model Performance on Test Set ---")
    print(f"Mean Squared Error (MSE): {mse:.4f}")
    print(f"R-squared (R2) Score: {r2:.4f}")

    # Visualization
    # Plot 1: Learning Curve (Cost vs. Iterations)
    plt.figure(figsize=(10, 6))
    plt.plot(range(model.n_iter), model.cost_history)
    plt.title('Learning Curve: Cost (MSE) vs. Iterations')
    plt.xlabel('Number of Iterations')
    plt.ylabel('Cost (MSE)')
    plt.grid(True)
    plt.savefig('learning_curve.png') # Save plot for the report
    print("\nSaved plot to learning_curve.png")

    # Plot 2: Actual vs. Predicted Values with Best Fit Line
    plt.figure(figsize=(10, 6))

    # The scatter plot of your model's actual vs. predicted results
    plt.scatter(y_test, y_pred, alpha=0.6, edgecolors='w', linewidth=0.5, label='Model Predictions')

    # --- Lines for Reference ---

    # 1. The Red 45-degree "Perfect Prediction" line
    perfect_line = np.linspace(min(y_test), max(y_test), 100)
    plt.plot(perfect_line, perfect_line, 'r--', linewidth=2, label='Perfect Prediction (45° Line)')

    # 2. The Green "Line of Best Fit" for your predictions
    # Calculate the coefficients (slope and intercept) of the best fit line
    m, c = np.polyfit(y_test, y_pred, 1)
    plt.plot(y_test, m * y_test + c, 'g-', linewidth=2, label='Best Fit Line')

    # --- Formatting ---
    plt.title('Actual Values vs. Predicted Values on Test Set')
    plt.xlabel('Actual Values (House Price)')
    plt.ylabel('Predicted Values (House Price)')
    plt.legend()
    plt.grid(True)
    plt.savefig('actual_vs_predicted_with_fit.png')
    print("Saved plot with best fit line to actual_vs_predicted_with_fit.png")


    # Experimenting with Different Learning Rates ---
    print("\n--- Running Bonus: Learning Rate Comparison ---")

    # Define the learning rates to experiment with
    learning_rates = {
        "Too High (α=0.5)": 0.5,
        "Just Right (α=0.01)": 0.01,
        "Too Low (α=0.0001)": 0.0001
    }

    plt.figure(figsize=(10, 6))

    for name, lr in learning_rates.items():
        # Train a new model for each learning rate
        model = LinearRegression(learning_rate=lr, n_iterations=1000)
        model.fit(X_train_scaled, y_train)

        # Plot the cost history for this model
        plt.plot(range(model.n_iter), model.cost_history, label=name)
        print(f"Finished training for learning rate: {lr}")

    # Formatting the comparison plot
    plt.title('Impact of Learning Rate on Convergence')
    plt.xlabel('Number of Iterations')
    plt.ylabel('Cost (MSE)')
    plt.legend()
    plt.grid(True)
    # Set a y-axis limit to keep the plot readable, as the "Too High" rate will diverge
    plt.ylim(0, 20)
    plt.savefig('learning_rate_comparison.png')
    print("\nSaved comparison plot to learning_rate_comparison.png")